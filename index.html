<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">

  <title>CS 7641 Machine Learning: Product Recommendation Systems</title>

  <link rel="canonical" href="http://localhost:4000/">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
  <link rel="stylesheet" type="text/css" href="/santander/css/main.css" >
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

  <!-- <link rel="shortcut icon" href="/classes/AY2020/cs7643_fall/images/icon.png"> -->

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    processEscapes: true
  }
});
</script>
</head>


<body>
  <nav class="navbar">
    <div class="navbar-header">
      <a class="navbar-brand" href="https://paulesta55.github.io/santander/">CS 7641 Machine Learning: Product Recommendation Systems</a>
      <div style="margin-top: 5px; color:white;">Paul Estano & Gregory Hessler</div>
    </div>
  </nav>
  <div class="container">
    <div class="content">
      <div style="padding-top: 15px;" class="card">

        <h1 id="cs-4803--7643-deep-learning">CS 7641 Machine Learning: Product Recommendation Systems</h1>

        <h3 style="font-weight: 300;">Paul Estano & Gregory Hessler</h3>

        <hr />

        <div class="row">
          <ul class="nav">
            <li class="nav-item">
              <a class="nav-link" href="#intro">Introduction</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#matrix">Matrix Factorization Techniques</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#supervised">Supervised Classifications</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#discussion">Results</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#discussion">Discussion</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#conclusion">Conclusion</a>
            </li>
          </ul>
        </div>

      </div>

      <p><a name="intro"></a></p>
      <div class="card">

        <h3>Introduction</h3>
        <p>A few years ago, recommendation systems attracted attention during the Netflix recommendations challeng\cite{netflix}. The purpose of this competition was to build a system for better user recommendations for the video streaming service. Recommendation systems have since become popular for many applications such as online marketplaces, music streaming services or social networks</p>

        <h4>Santander Product Recommendation</h4>
        <p>Our project focuses with Santander Product Recommendation dataset. This dataset contains 1.5  year  of Santander customer  behavioral data.Concretely, it is a CSV file with each row containing 46 indicators for one user for a specific month. 23 of these indicators are binary variables representing financial product bought by the customer.</p>
        <p>The first challenge with this dataset is that a significant part of the data is missing. Hence, we spent some time working on "filling holes" in the data</p>

        <p>The metric used to evaluate our model performance will be the Mean Average Precisiondefined as follows:
          $$ MAP@7=\frac{1}{|U|}\sum_{u=1}^{|U|}\frac{1}{min(m,7)}\sum_{k=1}^{min(n,7)} P(k),$$
      </div>

      <p><a name="matrix"></a></p>
      <div class="card">
        <h3>Matrix Factorization Techniques</h3>
        <p>Here the objectif is to express latent variables respresenting the taste of the users and categories representing similarities between the products. We mainly use 2 unsupervised algorithms: SVD and Dictionary Learning.
        These algorithms aim at expressing latent variables representing abstract categories of products and users tastes by factorizing a matrix of ratings.</p>
        <P>The rating matrix we use is made of the products used by each user at month $n-1$ (month preceding the test set data). It is an $m$ by $n$ sparse matrix with $m$ the number of users and $n$ the number of products. By factoring this matrix in 2 other $U$ and $V$ matrices in a lower dimension and reconstructing it by multiplying the 2 generated matrices, we can express users tastes for products they have not tried yet.</p>
        
        <img src="/santander/img/svd.PNG" width="730" height="230">
        
        <h4>SVD</h4>
        <p>The use of SVD to build a recommendation system has been introduced by Simon Funk during the Netflix Recommendation challenge. In his paper (Funk SVD), Funk uses an iterative algorithm to compute the SVD. Here we just use numpy built in SVD function to extract latent variables representing abstract categories of financial product
        $$M = U\ S\ V$$
        If \(R\) is an $m$ by $n$ matrix containing the ratings of every user in each row, then $U$ is an $m$ by $r$ matrix containing the tastes of the users and $V$ is an $r$ by $n$ the latent variables encoding categories of products. We select the $k$ most important components by analyzing the $r$ singular values contained in $S$</p>
        <p>This model is sensitive to the number of components chosen. Hence, we fine-tune $k$ and fine the following curve.</p>
        <img src="/santander/img/svd_k_rel.png" width="450" height="300">
        <h4><a href="https://www.di.ens.fr/sierra/pdfs/icml09.pdf">Dictionary Learning</a></h4>
        <p>Here we find $U$ and $V$ by solving the following optimization problem:
        $$argmin_{(U,V)}\ 0.5\ || M - U V ||_2^2 + alpha * || U ||_1
            
            \\with\ || V_k ||_2 = 1\ \text{for all  $0 <= k < n_{components}$}$$

        Several algorithms can be used to solve this problem including <a href="http://statweb.stanford.edu/~tibs/ftp/lars.pdf">LARS (Least Angle Regression)</a> or Orthogonal Matching Pursuit algorithm.
        </p>
      </div>




      <p><a name="supervised"></a></p>
      <div class="card">

        <h3>Supervised Classifications</h3>
      <p>Here we consider this recommendation problem as a classification problem. Therefore, we try to predict the best recommendations for every user for the month of the test set. For every user we predict a probability for each product and select the 7 product with highest probability</p>

      <h4>Ridge Classifier</h4>
      <p>
      We first use a classical regularized logistic regression technique. We choose $ \alpha=10 $ using cross-validation.
      For this technique we find a Mean Average Precision @ 7 of 0.00310. </p>
      
      <h4>Gradient Boosting</h4>
      <p>
      Gradient boosting is an ensemble method used to learn trees. The main idea is to learn the decision function $\hat{F}(x)$ as a weighted sum of decision trees $h_i$. 
$$\hat{F}(x) = \sum_{i=1}^M\gamma_i h_i(x) + const$$</p>
      Weights are updated at every iterations using gradient descent. 
      Using XGBoost implementation we find an MAP of 0.02689 which is our best result. Our parameters are a maximum depth of 8 for every tree and a learning rate of 0.05.

      </div>


      <p><a name="discussion"></a></p>
      <div class="card">
        <h3>Discussion</h3>
      
        <p>Our best performing model is the gradient boosting model. However, the depth of the trees and the number of features make the model complicated to explain as it can be seen below. </p>
        <img src="/santander/img/tree.png" width="800" height="1000">
      </div>
      <p><a name="conclusion"></a></p>
      <div class="card">

       <h3>Conclusion</h3>

     </div>

     <footer style="font-weight: 300;">
      <hr>
      &#169; 2020 Georgia Tech
    </footer>


    <!-- mathjax -->
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  </body>
  </html>
